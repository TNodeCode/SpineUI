{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SpineUI","text":"<p>Welcome to SpineUI, a detection and visualization app for dentritic spine detection.</p>"},{"location":"Annotation%20Formats/1-MMDetection%20Tracking/","title":"MMDetection Tracking Dataset","text":"<p>Sample annotation file</p> <pre><code>{\n    \"categories\": [\n        {\n            \"id\": 1,\n            \"name\": \"pedestrian\"\n        }\n    ],\n    \"videos\": [\n        {\n            \"id\": 1,\n            \"name\": \"aid052N1D3_tp1_stack1_default_aug_False_epoch_19_theta_0.5_delta_0.1_Test\",\n            \"fps\": 1,\n            \"width\": 512,\n            \"height\": 512\n        },\n        ...\n    ],\n    \"images\": [\n        {\n            \"id\": 1,\n            \"video_id\": 1,\n            \"file_name\": \"aid052N1D3_tp1_stack1_default_aug_False_epoch_19_theta_0.5_delta_0.1_Test\\\\img\\\\000001.png\",\n            \"height\": 512,\n            \"width\": 512,\n            \"frame_id\": 0,\n            \"mot_frame_id\": 1\n        },\n        ...\n    ],\n    \"annotations\": [\n        {\n            \"category_id\": 1,\n            \"bbox\": [\n                22.0,\n                129.0,\n                18.0,\n                14.0\n            ],\n            \"area\": 252.0,\n            \"iscrowd\": false,\n            \"visibility\": 1.0,\n            \"mot_instance_id\": 0,\n            \"mot_conf\": 1.0,\n            \"mot_class_id\": 0,\n            \"id\": 1,\n            \"image_id\": 1,\n            \"instance_id\": 0\n        },\n        ...\n    ],\n    \"num_instances\": 784\n}\n</code></pre>"},{"location":"Annotation%20Formats/1-MMDetection%20Tracking/#json-annotation-file-structure","title":"JSON annotation file structure","text":"<pre><code>erDiagram\n    DATASET {\n        int num_instances \"Number of distinct annotation instance ids\"\n    }\n    CATEGORY }o--|| DATASET : has\n    VIDEO }o--|| DATASET : has\n    IMAGE }o--|| DATASET : has\n    ANNOTATION }o--|| DATASET : has</code></pre>"},{"location":"Annotation%20Formats/1-MMDetection%20Tracking/#category-entities","title":"Category entities","text":"<pre><code>erDiagram\n    CATEGORY {\n        int id PK \"The category ID starting from 1\"\n        string name \"The category name\"\n    }</code></pre>"},{"location":"Annotation%20Formats/1-MMDetection%20Tracking/#video-entities","title":"Video entities","text":"<pre><code>erDiagram\n    VIDEO {\n        int id PK \"The sequence / video ID\"\n        string name \"The sequence / video name\"\n        fps int \"frames per second of the video\"\n        width int \"frame / image width\"\n        height int \"frame / image height\"\n    }</code></pre>"},{"location":"Annotation%20Formats/1-MMDetection%20Tracking/#image-entities","title":"Image entities","text":"<pre><code>erDiagram\n    IMAGE {\n        id int PK \"Image ID starting from 1\"\n        video_id int \"FK Video ID\"\n        file_name string \"Image filename (relative to train directory)\"\n        height int \"Image height\"\n        width int \"Image width\"\n        frame_id int \"Frame ID (starting from 0)\"\n        mot_frame_id int \"MOT frame ID (starting from 1)\"\n    }</code></pre>"},{"location":"Annotation%20Formats/1-MMDetection%20Tracking/#annotation-entities","title":"Annotation entities","text":"<pre><code>erDiagram\n    ANNOTATION {\n        id int PK \"Annotation ID (starting from 1)\"\n        category_id int FK \"ID of the category of this bounding box\"\n        bbox list[int] \"Bounding box in XYXY format\"\n        area int \"Bounding box area (width * height)\"\n        iscrowd bool \"Always false\"\n        visibility float \"Always 1.0\"\n        mot_instance_id int \"Instance ID of this object (every distinct object has its own ID)\"\n        mot_conf float \"Confidence score (always 1.0 in GT)\"\n        mot_class_id int \"Class ID (same as category_id minus 1)\"\n        instance_id int \"Instance ID\"\n    }</code></pre>"},{"location":"Annotation%20Formats/1-MMDetection%20Tracking/#convert-mot-to-mmdetection-coco-format","title":"Convert MOT to MMDetection COCO format","text":"<p>MMDetection contains a script <code>tools/dataset_converters/mot2coco.py</code> that can convert the MOT annotation format to MMDetection COCO format. First place your dataset in the <code>data/MOT17</code> directory with the following structure:</p> <pre><code>train\n|- &lt;stack_train1&gt;\n  |- det\n    |- det.txt\n  |- gt\n    |- gt_half-train.txt\n    |- gt_half-val.txt\n    |- gt.txt\n  |- img\n    |- 000001.png\n    |- 000002.png\n    |- 000003.png\n    |- ...\n  |- seqinfo.ini\n|- &lt;stack_train2&gt;\n  |- ...\n...\nval\n|- &lt;stack_val1&gt;\n...\ntest\n|- &lt;stack_test1&gt;\n...\n</code></pre> <p>Next call the following Python script to create COCO tracking and REID datasets.</p> <pre><code># Create COCO tracking dataset\npython ./tools/dataset_converters/mot2coco.py -i ./data/MOT17 -o ./data/MOT17/annotations --split-train --convert-det\n\n# Create REID annotations\npython ./tools/dataset_converters/mot2reid.py -i ./data/MOT17/ -o ./data/MOT17/reid --val-split 0.2 --vis-threshold 0.3\n</code></pre>"},{"location":"Annotation%20Formats/2-MOT%20Dataset%20Format/","title":"MOT Dataset annotations","text":"<p>Within your MOT project there should be a directories named <code>data/MOT17/&lt;train|test&gt;</code> with the following structure:</p> <pre><code>data/MOT17\n|- &lt;train|test&gt;\n|-|- &lt;stack_name_1&gt;\n|-|-|- gt\n|-|-|-|- gt.txt\n|-|-|- img\n|-|-|-|- 000001.png\n|-|-|-|- 000002.png\n|-|-|-|- [...]\n|-|-|- seqinfo.ini\n|-|- &lt;stack_name_2&gt;\n|-|-|- [...]\n|-|- &lt;stack_name_3&gt;\n|-|-|- [...]\n|-|- seqmaps.txt\n</code></pre> <p>The file <code>seqmaps.txt</code> contains the names of all stacks in the following CSV format:</p> <pre><code>name\n&lt;stack_name_1&gt;\n&lt;stack_name_2&gt;\n[...]\n</code></pre> <p>The first line is the header line and then for each stack a new line containing only the name of the stack is added. The stack names are the same as the directory names within <code>&lt;train|test&gt;</code>.</p> <p>The file <code>gt.txt</code> contains the ground truth in the following format:</p> <p><code>&lt;frame&gt;,&lt;object_id&gt;,&lt;x0&gt;,&lt;y0&gt;,&lt;width&gt;,&lt;height&gt;,&lt;confidence&gt;,&lt;x&gt;,&lt;y&gt;,&lt;z&gt;</code></p> <p>Here is an example of how this could look like:</p> <pre><code>1,3,230,479,13,11,1.0,-1,-1,-1\n2,3,230,478,15,17,1.0,-1,-1,-1\n2,2,225,435,14,16,1.0,-1,-1,-1\n</code></pre> <p>You can find more about the MOT17 format here: https://motchallenge.net/instructions/</p> <p>The file <code>seqinfo.ini</code> contains the info about the stack in the following format:</p> <pre><code>[Sequence]\nname=&lt;stack_name&gt;\nimDir=img\nframeRate=1\nseqLen=20\nimWidth=512\nimHeight=512\nimExt=.png\nseqLength=20\n</code></pre>"},{"location":"Annotation%20Formats/3-MOTRv2/","title":"MOTRv2 Annotations","text":""},{"location":"Annotation%20Formats/3-MOTRv2/#detector-results-format","title":"Detector Results Format","text":"<p>Results from a detection model are stored in a JSON file named <code>det_db_motrv2.json</code>. The file contains detections in the following schema:</p> <pre><code>{\n    \"&lt;filename1&gt;\": [\n        \"x0,y0,w,h,score\",\n        \"x0,y0,w,h,score\",\n        ...\n    ],\n    \"&lt;filename2&gt;\": [\n        \"x0,y0,w,h,score\",\n        \"x0,y0,w,h,score\",\n        \"x0,y0,w,h,score\",\n        ...\n    ],\n    ...\n}\n</code></pre> <p>An example could look like this:</p> <pre><code>{\n  \"DanceTrack/train/dancetrack0016/img1/00001863.txt\": [\n    \"1088.1,368.6,221.4,543.4,0.98\\n\",\n    \"826.2,361.1,217.4,481.3,0.97\\n\",\n    \"735.8,390.1,162.0,394.9,0.97\\n\",\n    \"495.5,324.0,149.8,614.2,0.97\\n\",\n    \"968.0,403.0,175.5,387.4,0.96\\n\",\n    \"398.9,357.8,141.1,484.7,0.92\\n\"\n  ],\n  \"DanceTrack/train/dancetrack0016/img1/00001877.txt\": [\n    \"484.3,349.0,189.0,573.1,0.98\\n\",\n    \"1119.8,371.6,180.9,538.3,0.97\\n\",\n    \"797.9,370.6,294.3,471.2,0.97\\n\",\n    \"978.8,407.0,147.2,377.3,0.95\\n\",\n    \"763.4,386.1,135.0,397.6,0.94\\n\",\n    \"411.4,361.1,144.4,482.6,0.91\\n\"\n  ]\n}\n</code></pre>"},{"location":"Annotation%20Formats/4-CrowdHuman/","title":"CrowdHuman Dataset","text":"<p>CrowdHuman is a popular standard dataset for training MOT models. Many MOT models accept datasets in the CrowdHuman annotation format. We will explain this annotation format in this article.</p>"},{"location":"Annotation%20Formats/4-CrowdHuman/#tracking-annotations","title":"Tracking Annotations","text":"<p>Tracking ground truth data is stored in a ODGT format, which is basically a JSON file but with each line containing a JSON document instead of the whole file containing a single JSON document. If you want to read this annotation file with Python you have to split it into lines and then parse each line as a JSON document. This is the schema of the tracking annotation ground truth.</p> <pre><code>JSON{\n    \"ID\" : image_filename,\n    \"gtboxes\" : [gtbox], \n}\n\ngtbox{\n    \"tag\" : \"person\" or \"mask\", \n    \"vbox\": [x, y, w, h],\n    \"fbox\": [x, y, w, h],\n    \"hbox\": [x, y, w, h],\n    \"extra\" : extra, \n    \"head_attr\" : head_attr, \n}\n\nextra{\n    \"ignore\": 0 or 1,\n    \"box_id\": int,\n    \"occ\": int,\n}\n\nhead_attr{\n    \"ignore\": 0 or 1,\n    \"unsure\": int,\n    \"occ\": int,\n}\n</code></pre> <p>An example of these annotations could look like this:</p> <pre><code>{\n  \"ID\": \"273271,c9db000d5146c15\",\n  \"gtboxes\": [\n    {\n      \"fbox\": [\n        72,\n        202,\n        163,\n        503\n      ],\n      \"tag\": \"person\",\n      \"hbox\": [\n        171,\n        208,\n        62,\n        83\n      ],\n      \"extra\": {\n        \"box_id\": 0,\n        \"occ\": 0\n      },\n      \"vbox\": [\n        72,\n        202,\n        163,\n        398\n      ],\n      \"head_attr\": {\n        \"ignore\": 0,\n        \"occ\": 0,\n        \"unsure\": 0\n      }\n    },\n    ...\n}\n{\n  \"ID\": \"273271,c9db000d5146c15\",\n  \"gtboxes\": [\n    ...\n  ]\n  ...\n}\n...\n</code></pre> <p>The filennames are denoted without file extensions.</p> <p>You can find more about the annotation format here: https://www.crowdhuman.org/download.html or in the paper here: https://arxiv.org/pdf/1805.00123</p> <p>There are three different bounding boxes for each object. The full body box (fbox) contains the complete object, whether it is partially hidden or not. You can see examples of full body boxes below. They are are denoted with continuous, thick lines. The visible region boxes (vbox) contain only the visible part of an object. Vboxes always have an intersection with fboxes of 100%. Third there are head bounding boxes (hbox) which contain the head of a person. They also have an inttersection of 100% with the corresponding fboxes.</p> <p> source: https://www.crowdhuman.org</p>"},{"location":"Annotation%20Formats/4-CrowdHuman/#transfering-box-format-to-other-datasets","title":"Transfering box format to other datasets","text":"<p>If you want to annotate your own dataset with this annotation format and your dataset doesn't contain persons but other objects and objects event aren't hidden just use teh same bounding box coordinates for all three bounding box types. You should also set the attribute <code>gtboxes.headattr.ignore</code> to <code>1</code>.</p>"},{"location":"Annotation%20Formats/5-Trackformer/","title":"Trackformer Annotations","text":"<p>Trackformer uses a modified JSON file format for annotating ground truth for multi-object tracking. In this article we will explain this annotation format.</p>"},{"location":"Annotation%20Formats/5-Trackformer/#annotation-format","title":"Annotation format","text":"<p>Sample annotation file</p> <pre><code>{\n    \"type\": \"instances\",\n    \"categories\": [\n        {\n            \"supercategory\": \"spine\",\n            \"name\": \"spine\",\n            \"id\": 1\n        },\n        ...\n    ],\n    \"images\": [\n        {\n            \"file_name\": \"aid052N1D1_tp1_stack2_layer001.png\",\n            \"height\": 512,\n            \"width\": 512,\n            \"id\": 0,\n            \"first_frame_image_id\": 0,\n            \"seq_length\": 20,\n            \"frame_id\": 0\n        },\n        ...\n    ],\n    \"annotations\": [\n        {\n            \"id\": 0,\n            \"category_id\": 1,\n            \"image_id\": 3,\n            \"seq\": \"aid052N1D1_tp1_stack2\",\n            \"track_id\": 0\n            \"bbox\": [\n                337,\n                473,\n                23,\n                21\n            ],\n            \"area\": 483,\n            \"segmentation\": [],\n            \"ignore\": 0,\n            \"visibility\": 1.0,\n            \"iscrowd\": 0,\n        },\n        ...\n    ],\n    \"sequences\": [\n        \"aid052N1D1_tp1_stack2\",\n        ...\n    ],\n    \"frame_range\": {\n        \"start\": 0.0,\n        \"end\": 1.0\n    }\n}\n</code></pre>"},{"location":"Annotation%20Formats/5-Trackformer/#json-annotation-file-structure","title":"JSON annotation file structure","text":"<pre><code>erDiagram\n    DATASET {\n        string type \"Dataset type ('instances' for tracking)\"\n        sequences list[string] \"List of sequence names\"\n        frame_range object \"Object describing frame range\"\n    }\n    CATEGORY }o--|| DATASET : has\n    IMAGE }o--|| DATASET : has\n    ANNOTATION }o--|| DATASET : has\n    ANNOTATION }|--|| IMAGE : has\n    ANNOTATION }|--|| CATEGORY : has</code></pre>"},{"location":"Annotation%20Formats/5-Trackformer/#category-entities","title":"Category entities","text":"<pre><code>erDiagram\n    CATEGORY {\n        int id PK \"The category ID starting from 1\"\n        string supercategory \"Name of the supercategory (use the same name as for 'name')\"\n        string name \"The category name\"\n    }</code></pre>"},{"location":"Annotation%20Formats/5-Trackformer/#image-entities","title":"Image entities","text":"<pre><code>erDiagram\n    IMAGE {\n        id int PK \"Image ID starting from 1\"\n        file_name string \"Image filename (relative to train directory)\"\n        height int \"Image height\"\n        width int \"Image width\"\n        frame_id int \"Frame ID (starting from 0)\"\n        first_frame_image_id int \"ID of the first frame in the sequence\"\n        seq_length int \"Number of frames in the corresponding sequence\"\n    }</code></pre>"},{"location":"Annotation%20Formats/5-Trackformer/#annotation-entities","title":"Annotation entities","text":"<pre><code>erDiagram\n    ANNOTATION {\n        id int PK \"Annotation ID (starting from 1)\"\n        category_id int FK \"ID of the category of this bounding box\"\n        image_id int FK \"ID of the image\"\n        seq string \"Sequence name\"\n        track_id int \"ID of track in the sequence (starting from 0)\"\n        bbox list[int] \"Bounding box in XYWH format\"\n        segmentation list[int] \"Segmentation mask polygon\"\n        area int \"Bounding box area (width * height)\"\n        iscrowd int \"Always 0\"\n        ignore int \"Always 0\"\n        visibility float \"Object visibility\"\n    }</code></pre>"},{"location":"Annotation%20Formats/5-Trackformer/#convert-csv-annotations-to-trackformer-annotations","title":"Convert CSV annotations to Trackformer annotations","text":"<p>We have created a script that can transform tracking annotations in a CSV format described below into the Trackformer JSON annotation format. First you need to create the following directory structure in your <code>trackformer</code> directory:</p> <pre><code>data\n|- spine_detection\n  |- annotations\n    |- test.csv\n    |- train.csv\n    |- val.csv\n  |- test\n  |- train\n  |- val\n</code></pre> <p>Place your images in the <code>train</code>, <code>val</code> and <code>test</code> directories. Then add your annotations in the CSV files in the following format:</p> <pre><code>id,filename,width,height,class,score,xmin,ymin,xmax,ymax\n1,aid052N1D2_tp1_stack1_layer006.png,512,512,spine,1.0,445.0,243.0,458.0,263.0\n0,aid052N1D2_tp1_stack1_layer006.png,512,512,spine,1.0,339.0,232.0,353.0,245.0\n2,aid052N1D2_tp1_stack1_layer006.png,512,512,spine,1.0,326.0,254.0,344.0,270.0\n2,aid052N1D2_tp1_stack1_layer007.png,512,512,spine,1.0,327.0,254.0,345.0,270.0\n3,aid052N1D2_tp1_stack1_layer007.png,512,512,spine,1.0,464.0,278.0,488.0,306.0\n4,aid052N1D2_tp1_stack1_layer007.png,512,512,spine,1.0,496.0,54.0,512.0,76.0\n0,aid052N1D2_tp1_stack1_layer007.png,512,512,spine,1.0,338.0,230.0,356.0,245.0\n1,aid052N1D2_tp1_stack1_layer007.png,512,512,spine,1.0,443.0,241.0,460.0,261.0\n5,aid052N1D2_tp1_stack1_layer008.png,512,512,spine,1.0,217.0,208.0,236.0,248.0\n3,aid052N1D2_tp1_stack1_layer008.png,512,512,spine,1.0,463.0,278.0,485.0,304.0\n6,aid052N1D2_tp1_stack1_layer008.png,512,512,spine,1.0,259.0,233.0,270.0,248.0\n</code></pre> <p><code>id</code> is the object identity. It can occur multiple times in a sequence, but only once in a frame.</p> <p>Now call the following Python script to create JSON annotation files in the <code>annotation</code>directory:</p> <pre><code>$ python src/generate_coco_from_spine.py\n</code></pre> <p>After running this command you will find the annotation files <code>train.json</code>, <code>val.json</code> and <code>test.json</code> in the <code>annotations</code>directory.</p>"},{"location":"App/1-Intoduction/","title":"Introduction","text":""},{"location":"App/1-Intoduction/#software-requirements","title":"Software Requirements","text":"<p>For using SpineUI you need to have installed Docker on your machine.</p>"},{"location":"App/1-Intoduction/#how-to-start-the-application","title":"How to start the application","text":"<p>You can start the appliction by running the following command: <code>docker compose up -d</code>. When you run the appliaction Docker will download all the required images needed for running this application. It will download one image for the UI and one image per model that is being used.</p>"},{"location":"App/1-Intoduction/#shut-down-the-application","title":"Shut down the application","text":"<p>The application can be shut down by executing the command <code>docker compose down</code>.</p>"},{"location":"App/2-Datasets/","title":"Datasets","text":"<p>Managing datasets with SpineUI is straight forward. The configuration for your datasets is stored in <code>config/datasets.yml</code>.</p>"},{"location":"App/2-Datasets/#how-it-works","title":"How it works","text":"<p>First of all you need to place all your images somewhere on your local disk. It is recommended to use the <code>./datasets</code> directory in this project, but you could also choose any other directory. Next you need to adapt the configuration file. In the configuration file datasets look like this:</p> <pre><code>datasets:\n  - name: my_dataset_train\n    paths:\n      - ./datasets/my_dataset/train/*.png\n  - name: my_dataset_test\n    paths:\n      - ./datasets/my_dataset/test/*.png\n</code></pre> <p>For every dataset you need to add at least one entry to the <code>datasets</code> list. Every dataset needs to have a unique <code>name</code> and a list of <code>paths</code> where the images of that dataset are located. You can use glob expressions for images containing wildcards like <code>/path/image_day01_*.png</code>. With that syntax image directories can be used for multiple datasets. Also multiple paths can be added to a single dataset.</p>"},{"location":"App/2-Datasets/#dividing-datasets-into-stacks","title":"Dividing Datasets into Stacks","text":""},{"location":"App/2-Datasets/#using-regular-expressions-to-define-stacks","title":"Using regular expressions to define stacks","text":"<p>In the Dentritic Spine Detection Project we have to deal with 3D information. When pictures are taken with the two-photon microscope multiple images of the same 3D voume are taken. It makes sense to use a naming schema for those images, so that images of the same 3D volume can be linked by their name. For example let's assume that we give each stack a unique numeric ID and name the images as folows: <code>stack_&lt;stack_id&gt;_&lt;image_id&gt;.png</code>.</p> <p>Say that we have images of two stacks each containing three photos. So our dataset would have the following fienames:</p> <pre><code>stack_001_001.png\nstack_001_002.png\nstack_001_003.png\nstack_002_001.png\nstack_002_002.png\nstack_002_003.png\n</code></pre> <p>For each stack we need to create an entry of the <code>datasets.stacks</code> object in the configuration file. This object will have the keys <code>regex</code> and <code>stack_name</code>.</p> <p><code>regex</code> contains a regular expression that is applied to all images that are found in the dataset paths. If the regular expression matches a filename this image becomes part of the stack. An image can also be part of multiple stacks. Regular expressions should always start end end with the <code>.*</code> pattern, because otherwise some operating systems won't be able to match any filename. All variable parts of the filename shoud be replaced by groups, which are denoted with round brackets <code>()</code> in a regular expression. Within these brackets there is the pattern for the variable part of the filename. For example <code>[0-9]{1,3}</code> means that the numbers from zero to nine can appear at least one time and at most three times in this group.</p> <pre><code>datasets:\n  - name: spine_tracking\n    stacks:\n      patterns:\n        - regex: .*stack_([0-9]{1,3})_([0-9]{1,3}).*\n          stack_name: \"Stack $1\"\n</code></pre> <p><code>stack_name</code> contains the final name of the stack which is used for grouping the images. Every <code>$i</code> part of that name is replaced by the value of the i-th group of the regular expression. So in the given example the stack names for the images would be:</p> <pre><code>Stack 001\nStack 001\nStack 001\nStack 002\nStack 002\nStack 002\n</code></pre> <p>So this naming scheme will result in two different stack names you can choose from in the UI. If we would change the stack naming schema to <code>stack_name: Stack $1 Image $2</code> we would end up with six different stacks each containing a single image.</p>"},{"location":"App/2-Datasets/#multiple-patterns","title":"Multiple patterns","text":"<p>Now let's assume we add some more images to our dataset which have a different naming schema. The filenames of the new images look ike this:</p> <pre><code>S3I1\nS3I2\nS3I3\n</code></pre> <p>These filenames would not be matched by the pattern we defined before. What we can do is to add another pattern to our stack configuration like this:</p> <pre><code>datasets:\n  - name: spine_tracking\n    stacks:\n      patterns:\n        - regex: .*stack_([0-9]{1,3})_([0-9]{1,3}).*\n          stack_name: \"Stack $1\"\n        - regex: .*S([0-9]{1,3})I([0-9]{1,3}).*\n          stack_name: \"Stack $1\"\n</code></pre>"},{"location":"App/2-Datasets/#annotations","title":"Annotations","text":"<p>Now that we know how we can add datasets to our configuration and how to divide these datasets into stacks we need to add the annotations of our images to the dataset. SpineUI can handle multiple annotations for the same images, so that if for example multiple persons annotated the same data you can use SpineUI to compare those annotations.</p> <p>Annotations are configured under the <code>datasets.[*].annotations</code> key of a dataset object in the configuration file.</p>"},{"location":"App/2-Datasets/#adding-bounding-box-annotations-in-the-coco-format","title":"Adding bounding box annotations in the COCO format","text":"<p>If you want to add bounding boxes to your dataset in the COCO format add a new list item to the list in <code>datasets[*].annotations</code> with the following keys:</p> <p><code>name</code>: A name for the annotations which you can choose. This name will be displayed in the SpineUI app. </p> <p><code>type</code>: The annotation type, <code>coco</code> in this case.</p> <p><code>paths</code>: The COCO JSON annotation file. This mist be passed as a list, although we usually only have a single JSON annotation file. This is because other formats like Masks require multiple annotation sources.</p> <p>This is how bounding box annotations can look like in the configuration file.</p> <pre><code>datasets:\n  - name: spine_tracking\n    annotations:\n      - name: annotator01\n        type: coco\n        paths: \n          - ./datasets/spine/annotations/instances_train2017.json\n</code></pre>"},{"location":"App/2-Datasets/#adding-mask-annotations","title":"Adding mask annotations","text":"<p>SpineUI can also handle mask annotations. Mask annotations use black and white PNG images for storing the mask annotation. You need to place a PNG file with the same name as the dataset image in a directory and then add this directory name to <code>datasets[*].annotations.paths</code>. Mask annotation images can also be placed in multiple directories. Then you just need to add each path to the key mentioned above. Also the value of <code>datasets[*].annotations.type</code> must be set to <code>masks</code>.</p> <p>An annotation for masks could look like this:</p> <pre><code>datasets:\n  - name: spine_tracking\n    annotations:\n      - name: mask-annotations-01\n        type: masks\n        paths: \n          - ./datasets/spine/train-sam-b/*\n</code></pre>"},{"location":"App/2-Datasets/#training-results","title":"Training Results","text":"<p>There are two types of annotations which are distinguished in SpineUI: ground truth annotations and the annotations generated by a model. Annotations generated by a model are stored in CSV files and are places in the configuration under <code>datasets[*].detections</code>. Each detection has the following entries:</p> <p><code>name</code>: The name that should be used for the detections when displayed in SpineUI. It makes sense to use the model name here.</p> <p><code>paths</code>: Paths to CSV files containing the detected bounding boxes.</p> <p>A configuration for two training results could look like this:</p> <pre><code>datasets:\n  - name: spine_tracking\n    detections:\n      - name: Cascade RCNN\n        paths:\n          - ./detections/cascade_rcnn/cascade_rcnn_run*_spine_mixed_train.csv\n      - name: Faster RCNN\n        paths:\n          - ./detections/faster_rcnn/faster_rcnn_run*_spine_mixed_train.csv\n</code></pre>"},{"location":"App/3-REST-API/","title":"REST API","text":"<p>The SpineUI app provides a REST API for accessing detection and tracking models. We will give you an overview of how you can use this API in this article.</p>"},{"location":"App/3-REST-API/#endpoint-docs","title":"Endpoint /docs","text":"<p>The endpoint <code>/docs</code> provides an OpenAPI web interface for the REST API. OpenAPI is a documentaion and interaction standard for REST APIs that provides a GUI for interacting with the REST API.</p>"},{"location":"App/3-REST-API/#endpoint-available_models","title":"Endpoint /available_models","text":"<p>Call the <code>&lt;url&gt;/available_models</code> endpoint to see all the available models that can be used.</p>"},{"location":"App/MMDET%20Models/","title":"MMDET Models","text":"<p>In this article we show you how you can use models that were trained with the MMDET framework. For the spine detection task we trained four different models:</p> <ul> <li>Faster RCNN</li> <li>Cascade RCNN</li> <li>Deformable DETR</li> <li>CO-DETR</li> </ul>"},{"location":"App/MMDET%20Models/#model-implementations","title":"Model implementations","text":"<p>In MMDET each model implementation is described by a configuration file. Below you can see a list of all the models that have been trained on the spine dataset and are supported by the MMDET docker container.</p>"},{"location":"App/MMDET%20Models/#faster-rcnn","title":"Faster RCNN","text":"Model Configfile faster_rcnn faster_rcnn_r50_fpn_1x_coco faster_rcnn faster_rcnn_r101_fpn_1x_coco faster_rcnn faster_rcnn_x101_32x4d_fpn_1x_coco faster_rcnn faster_rcnn_x101_64x4d_fpn_1x_coco"},{"location":"App/MMDET%20Models/#cascade-rcnn","title":"Cascade RCNN","text":"Model Configfile Cascade RCNN cascade_rcnn_r50_fpn_1x_coco Cascade RCNN cascade_rcnn_r101_fpn_1x_coco Cascade RCNN cascade_rcnn_x101_32x4d_fpn_1x_coco Cascade RCNN cascade_rcnn_x101_64x4d_fpn_1x_coco"},{"location":"App/MMDET%20Models/#deformable-detr","title":"Deformable DETR","text":"Model Configfile Deformable DETR deformable_detr_r50_16x2_50e_coco Deformable DETR deformable_detr_refine_r50_16x2_50e_coco Deformable DETR deformable_detr_twostage_refine_r50_16x2_50e_coco"},{"location":"App/MMDET%20Models/#co-detr","title":"Co-DETR","text":"Model Configfile Co-DETR co_dino_5scale_swin_large_16e_o365tococo_spine"},{"location":"App/MMDET%20Models/#configure-the-mmdet-docker-container","title":"Configure the MMDET docker container","text":"<p>First we need to add the MMDET image to a docker compose file. Have a look at the <code>docker-compose.yml</code> file in this repository. The following section should be part of this file:</p> <pre><code>services:\n  mmdet:\n    image: tnodecode/spine-co-detr\n    hostname: spine-co-detr\n    ports:\n      - 8081:80\n    volumes:\n      - ./available_models.yml:/app/available_models.yml\n      - ./models/mmcv:/app/runs\n</code></pre> <p>This means when you run the docker environment with the command <code>docker compose up -d</code> a docker container based on the image <code>tnodecode/spine-co-detr</code> is started. It will run on port 8081 on your machine. Also two directories will be mapped into the container. First the <code>available_models.yml</code> file in the root directory of this repository will be mapped to the <code>/app/available_models.yml</code> directory inside of the container. Also the directory <code>./models</code> of this repository will be mapped to the <code>/app/runs</code> directory inside the container. This means if you have a model weight file on this machine placed in <code>./models/mmvc/my_weights.pth</code> it will be available in the container under <code>/app/runs/my_weights.pth</code>.</p>"},{"location":"App/MMDET%20Models/#updating-the-docker-image","title":"Updating the docker image","text":"<p>You can update the MMDET docker image by running th e command <code>docker pull tnodecode/spine-co-detr</code> in your terminal. Sometimes docker compose doen't check for newer versions of the docker images.</p>"},{"location":"App/MMDET%20Models/#configuring-the-available-models","title":"Configuring the available models","text":"<p>Next we need to tell the container which models we have trained and where the weight files of these models are. This is what the <code>available_models.yml</code> file is resposible for. For each available weight file we need to create an entry like the following one in this file:</p> <pre><code>faster-rcnn-r50:\n  config: ./configs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco.py\n  weights: ./runs/faster_rcnn_r50_fpn_1x_coco/epoch_9.pth\n</code></pre> <p>Each entry needs to have a unique key like <code>faster-rcnn-r50</code> in this example. The key can be chosen freely. Each key has an attribute named <code>config</code> and an attribute named <code>weights</code>. The <code>config</code> attribute points to the configuration file inside of the docker container. Usually in MMDET configuration files are places in the subdirectory <code>./configs/&lt;model_name&gt;/&lt;config_file&gt;.py</code>. Replace <code>&lt;model_name&gt;</code> with one of the supported model names from the table above and <code>&lt;config_file&gt;</code> with the corresponding configuration file name. For the <code>weights</code> parameter add the path relative to the <code>/app</code> directory of the weight file. As we explained earlier a weight file that is stored under <code>./models/mmcv/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/epoch_9.pth</code> in this repository will be mapped to the path <code>/app/runs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/epoch_9.pth</code> inside of the docker container, so the path relative to <code>/app</code> would be <code>./runs/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/epoch_9.pth</code>in this case.</p>"},{"location":"App/MMDET%20Models/#exploring-the-openapi-interface-of-the-docker-container","title":"Exploring the OpenAPI interface of the docker container","text":"<p>The REST-API that was built around the MMDET library supports a documentation format called OpenAPI. OpenAPI is a web interface that can be used for testing a REST-API. When the docker container is started this OpenAPI interface will be available at http://localhost:8081/docs.</p> <p></p>"},{"location":"App/MMDET%20Models/#get-all-available-models","title":"Get all available models","text":"<p>If you have added all your available model weight files to the <code>available_models.yml</code> file you can test if they are recognized by the docker container using the OpenAPI interface. Click on the <code>GET /available_models</code> section and then on the <code>Try it out</code> button and finally on the <code>Execute</code> button.</p> <p>You should then see the response of the REST-API which should be a list of all your keys that are defined in <code>available_models.yml</code>.</p> <p></p>"},{"location":"App/MMDET%20Models/#testing-model-inference","title":"Testing model inference","text":"<p>There is a second endpoint the REST-API provides that can accept an image and returns a list of bounding boxes. Click on the <code>POST /image_inference/{model_id}</code> section, then on <code>Try it out</code>. In the <code>model_id</code>field enter one of the names that were returned by the <code>/available_models</code> endpoint, which should correspond to the keys in <code>available_models.yml</code>. Next click on the file input and select one image that should be sent through the model. Finally click on the <code>Execute</code> button.</p> <p></p> <p>After a few seconds the model should return a list of detected bounding boxes which could look like this:</p> <p></p>"},{"location":"App/MMDET%20Models/#using-the-mmdet-models-in-spineui","title":"Using the MMDET models in SpineUI","text":"<p>First we need to tell SpineUI under which address the MMDET docker container can be found. Each docker container that provides a REST-API with routes for available models and image inference can be registered in the configuration file <code>./config/models.yml</code>.</p> <p>When using the default docker-compose file the MMDET contaier is reachable under http://localhost:8081 in the browser. But when SpineUI itself runs within a docker container <code>localhost</code> is not the same for SpineUI as for our browser. The docker containers can find other docker containers that were created by the same docker-compose file via the container hostnames, which means that we have to use the address <code>http://mmdet:80</code> in the configuration file.</p> <p>Add the MMDET REST-API to this configuration file:</p> <pre><code>models:\n  - name: MMDET Docker\n    url: http://mmdet:80\n</code></pre> <p>Now you can use the docker container in the <code>Model Infrerence</code> view of SpineUI.</p>"},{"location":"App/MMDET%20Models/#using-python-to-send-images-to-the-rest-api","title":"Using Python to send images to the REST-API","text":"<p>You can also use the REST-API interface in Python scripts like in the following example:</p> <pre><code>import requests\n\nfile_path = './path/to/image.png'\nmodel_name = 'faster-rcnn-r50'\n\nwith open(file_path, 'rb') as file:\n    url = f\"http://localhost:8081/image_inference/{model_name}\"\n    response = requests.post(url, files={'file': file})\n\ndata = response.json()\nbboxes = data['bboxes']\n\n# Iterating over bounding boxes\nfor bbox in bboxes:\n    filename = bbox['filename']\n    class_index = int(bbox['class_index'])\n    class_name = bbox['class_name']\n    xmin = int(bbox['xmin'])\n    xmax = int(bbox['xmax'])\n    ymin = int(bbox['ymin'])\n    ymax = int(bbox['ymax'])\n    score = float(bbox['score'])\n</code></pre>"},{"location":"App/Models/","title":"Models","text":"<p>SpineUI allows you to connect various object detection models via a REST API interface. In this article you will learn how you can add existing endpoints to SpineUI.</p>"},{"location":"App/Models/#how-it-works","title":"How it works","text":"<p>Model interfaces are managed via the <code>models.yaml</code> configuration file in the <code>config</code> directory. The content looks like this:</p> <pre><code>models:\n  - name: Yolo Docker\n    url: http://yolo:80\n  - name: Yolo Local\n    url: http://localhost:8000\n</code></pre> <p>You can add new models by adding a new object to the <code>models</code> key containing a unique <code>name</code> and the <code>url</code> of the REST API. If the REST API is running outside of a docker container you usually use the <code>localhost</code> domain, which refers to the network interface of your local machine. However if the REST API and the user interface both run in docker images and are connected via a docker network they can use the docker domain name system. The containers usually can be reached by their names, so if a docker container's name is yolo it can be reached via <code>http://yolo</code>.</p> <p>If your models needs weight files you can place them inside the <code>models</code> directory of this project. You can mount this directory into the docker container by adding it to the <code>volumes</code> key in the <code>docker-compose.yml</code> file.</p>"},{"location":"App/Object%20Tracking/","title":"Object Tracking","text":"<p>When you have trained a model and performed inference on the dataset you can use object tracking algorithms to detect object that appear in multiple frames of a stack. In this article we show you how this can be done using the CLI.</p>"},{"location":"App/Object%20Tracking/#naive-object-tracking","title":"Naive Object Tracking","text":"<p>The naive tracking algorithm uses the intersection over maximum metric to classify two bounding boxes of two different frames as the same object. You can run the naive tracking algorithm using the following command:</p> <pre><code>python cli.py naive-tracking \\\n    --dataset spine_mixed_train \\\n    --detections detections/faster_rcnn/faster_rcnn_run1_spine_mixed_train.csv \\\n    --output-dir detections/naive-tracking \\\n    --threshold 0.5\n</code></pre> <p>Parameters: <code>dataset</code> is one of the datasets defined in the <code>dataset.yml</code> configuration file. <code>detections</code> is a path to a CSV file containing detected bounding boxes. <code>output-dir</code> is the path where the results of the naive tracking algorithm should be stored. <code>threshold</code> is the minimum confidence score that should be reached for a bounding box</p> <p>The resulting output file contains the tracking results in the MOT17 format and will look like this:</p> <pre><code>1,0,33,196,13,14,0.987898051738739,-1,-1,-1\n2,0,34,196,14,14,0.9786934852600098,-1,-1,-1\n3,0,32,193,15,15,0.8812576532363892,-1,-1,-1\n4,0,34,196,14,14,0.9571372270584106,-1,-1,-1\n1,1,22,130,18,15,0.9855337738990784,-1,-1,-1\n</code></pre> <p>Each line represents a detected bounding box in the following format: <code>&lt;frame&gt;,&lt;object_id&gt;,&lt;x0&gt;,&lt;y0&gt;,&lt;w&gt;,&lt;h&gt;,&lt;score&gt;,&lt;x&gt;,&lt;y&gt;,&lt;z&gt;</code></p>"},{"location":"App/Object%20Tracking/#tracking-evaluation","title":"Tracking Evaluation","text":"<p>After the tracking CSV file is generated you can evaluate the tracking results agains the ground truth data with the following command:</p> <pre><code>python cli.py eval-tracking \\\n    --gt-folder datasets/MOT17/train \\\n    --detections detections/naive-tracking \\\n    --output-dir detections/naive-tracking-evaluation \\\n    --similarity-metric IoM\n</code></pre> <p>Parameters: <code>--gt-folder</code>: Ground truth data in MOT17 format <code>--detections</code>: Directory that contains the tracking result CSV files (one per stack) <code>--output-dir</code>: Directory where evaluation results should be stored <code>--similarity-metric</code>: Metric for computing similarity of bounding boxes (IoU or IoM)</p>"},{"location":"Developers/Docker/","title":"Docker Containers","text":""},{"location":"Developers/Docker/#how-to-build-the-spineui-docker-container","title":"How to build the SpineUI Docker container","text":"<p>If you want to build the docker container for SpineUI you can do this by executing the following command in the project's root directory where <code>Dockerfile</code> is stored.</p> <pre><code>$ docker build -t tnodecode/spineui .\n</code></pre> <p>When the command has finished your docker image is ready to be used.</p>"},{"location":"Developers/Docker/#uploading-the-docker-image-to-docker-hub","title":"Uploading the docker image to Docker Hub","text":"<p>If you want to make your docker image publically available you first need to create an account for Docker Hub (https://hub.docker.com). Next you need to connect Docker on your local machine to your Docker Hub account by running <code>docker login</code>. You will need to enter your password. After that you have to tag your Docker image with the following naming schema: <code>&lt;username&gt;/&lt;image_name&gt;</code>. So if your username is <code>johndoe</code> and you want to publish an image with the name <code>abcde</code> you would first need to build that image on your machine by running <code>docker build -t johndoe/abcde</code> and then uploading it to Docker Hub by running <code>docker push johndoe/abcde</code>. After the image is uploaded everyone can use your image by doownloading it via the command <code>docker pull johndoe/abcde</code>.</p>"},{"location":"Developers/Docker/#updating-existing-images","title":"Updating existing images","text":"<p>For updating existing images you need to perform the same steps as for uploading new images. Docker will find out which image layers in the Docker registry need to be updated and will only upload those layers to the Docker Hub registry.</p>"},{"location":"Developers/Miniconda/","title":"Miniconda","text":"<p>Miniconda is a toll which helps you manage multiple python environments.</p> <p>Website: https://docs.anaconda.com/miniconda/</p>"},{"location":"Developers/Miniconda/#install-miniconda","title":"Install Miniconda","text":"<p>Here is how you can install Miniconda on your machine.</p> <pre><code>mkdir -p ~/miniconda3\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/miniconda3/miniconda.sh\nbash ~/miniconda3/miniconda.sh -b -u -p ~/miniconda3\nrm ~/miniconda3/miniconda.sh\n\n# On Linux\n~/miniconda3/bin/conda init bash\n\n# on Mac\n~/miniconda3/bin/conda init zsh\n</code></pre> <p>Close your terminal after executing these installation commands and reopen it. Now your terminal should show the name of the currently activated conda environment in brackets. The default environment's name is <code>base</code>.</p>"},{"location":"Developers/Miniconda/#create-a-new-environment","title":"Create a new environment","text":"<p>Run the following command to create and activate a new conda environment. After you have activated the new conda environment all the packages that are installed via the pip package manager are only available in this specific environment. This is really helpful if you want to use multiple PyTorch versions on the same machine.</p> <pre><code>conda create -y -n my-env python==3.12 pip\nconda activate my-env\n</code></pre>"},{"location":"Developers/Slurm/","title":"SLURM","text":"<p>SLURM (Simple Linux Utility for Resource Management) is an open-source workload manager that is widely used in high-performance computing (HPC) environments, such as clusters and supercomputers. It manages and schedules jobs (tasks or processes) on these systems, ensuring that resources like CPU, memory, and GPUs are allocated efficiently and fairly among users.</p>"},{"location":"Developers/Slurm/#create-a-script","title":"Create a script","text":"<p>Running a script on a SLURM cluster involves several steps, including preparing your script, creating a SLURM job submission script, and submitting it to the SLURM workload manager. Here\u2019s a step-by-step guide:</p> <p>First, make sure you have the script you want to run. This could be a Python script, a shell script, or any other executable script. For example, let\u2019s assume you have a Python script named <code>my_script.py</code>.</p> <p>To run your script on a SLURM cluster, you need to create a job submission script, which is a simple shell script containing SLURM directives that specify how your job should be executed.</p> <p>Here\u2019s an example of a SLURM job submission script for <code>my_script.py</code>:</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=my_job         # Job name\n#SBATCH --output=output_%j.txt    # Standard output and error log (%j will be replaced by job ID)\n#SBATCH --ntasks=1                # Run a single task (useful for serial jobs)\n#SBATCH --time=01:00:00           # Time limit hrs:min:sec (1 hour)\n#SBATCH --mem=4GB                 # Memory required per node (4 GB)\n#SBATCH --partition=normal        # Partition (queue) to submit to\n\n# Load any necessary modules\nmodule load python/3.8\n\n# Run your Python script\npython my_script.py\n</code></pre> <p>Note</p> <p>Ask your system administrator for the partition name you should use and the names of the available modules.</p>"},{"location":"Developers/Slurm/#run-a-script","title":"Run a script","text":"<p>Once you\u2019ve created your job submission script (e.g., submit.sh), you can submit it to the SLURM scheduler using the sbatch command:</p> <pre><code>$ sbatch submit.sh\n</code></pre>"},{"location":"Developers/Slurm/#inspect-progress-of-your-jobs","title":"Inspect progress of your jobs","text":"<p>After submitting, SLURM will assign your job a unique job ID and place it in a queue. The job will start running when the required resources are available. You can monitor the status of your job using the <code>squeue</code> command:</p> <pre><code>$ squeue -u your_username\n</code></pre> <p>If you are interested in all jobs that run on a specific partition you can get these with the following command:</p> <pre><code>$ squeue -p partition_name\n</code></pre> <p>This can be helpful if you want to see how many capacity is left on this partition. For example if you want to train a deep learning model on GPUs you can see how many jobs are currently running that use those GPUs.</p> <p>This will show the status of all your jobs. The main states are:</p> <ul> <li>PD (Pending): Waiting in the queue.</li> <li>R (Running): Currently executing.</li> <li>CG (Completing): Finishing up.</li> <li>CD (Completed): Finished successfully.</li> </ul> <p>When your job completes, SLURM will write the output and any errors to the file you specified with the <code>--output</code> directive (<code>output_%j.txt</code>). You can inspect this file to see the results of your script.</p>"},{"location":"Developers/Slurm/#canceling-a-job","title":"Canceling a Job","text":"<p>If you need to cancel a job, use the scancel command followed by the job ID:</p> <pre><code>scancel &lt;job_id&gt;\n</code></pre>"},{"location":"Developers/Spack%20Package%20Manager/","title":"Spack Package Manager","text":"<p>Spack is a package manager that allows you to install multiple versions of CUDA and other libraries on your cluster.</p> <p>see: https://csc.uni-frankfurt.de/wiki/doku.php?id=public:usage:spack</p>"},{"location":"Developers/Spack%20Package%20Manager/#install-spack-manager","title":"Install Spack Manager","text":"<ol> <li>Clone GitHub Repository: <code>git clone https://github.com/spack/spack</code></li> <li>Navigate into spack directory: <code>cd spack</code></li> <li>Make Spack command available in the terminal: <code>. spack/share/spack/setup-env.sh</code></li> </ol> <p>Now the <code>spack</code> command should be available in the terminal.</p>"},{"location":"Developers/Spack%20Package%20Manager/#install-cuda-modules","title":"Install CUDA modules","text":"<ol> <li>Show all available CUDA versions: <code>spack info cuda</code></li> <li>Install a specific CUDA version: <code>spack install cuda@11.8.0</code></li> <li>Run <code>spack module tcl refresh</code> to update the available modules</li> <li>Run <code>module avail</code> and you should see an output similar to this one:</li> </ol> <pre><code>------------ /scratch/dldevel/&lt;your-username&gt;/spack/share/spack/modules/linux-almalinux9-skylake_avx512 -------------\ncuda/11.8.0-gcc-11.3.1-szmxwyv\n</code></pre> <p>Now you can load CUDA 11.8.0 in your scripts using the following line:</p> <pre><code>module load cuda/11.8.0-gcc-11.3.1-szmxwyv\n</code></pre> <p>There cannot be loaded two CUDA modules at once. If you are unable to load the CUDA 11.8.0 module due to CUDA 12.3.0 already being loaded you may execute the following command first:</p> <pre><code>module unload cuda/11.8.0-gcc-11.3.1-szmxwyv\n</code></pre>"},{"location":"Developers/Spack%20Package%20Manager/#use-different-cuda-versions-in-your-scripts","title":"Use different CUDA versions in your scripts","text":"<p>Create two files:</p> <p><code>nvcc_12.sh</code></p> <pre><code>#!/bin/bash\n\n#SBATCH --job-name=nvcc-check\n#SBATCH --output=output_sbatch/%j.out\n#SBATCH --partition=gpu2\n#SBATCH --gres=gpu:1\n#SBATCH --account=dldevel\n#SBATCH --mem=1G\n#SBATCH --time=00:01:00\n\nmodule load nvidia/cuda/12.3.0\nnvcc --version\necho \"Finished execution\"\n</code></pre> <p><code>nvcc.11.sh</code></p> <pre><code>#!/bin/bash\n\n#SBATCH --job-name=nvcc-check\n#SBATCH --output=output_sbatch/%j.out\n#SBATCH --partition=gpu2\n#SBATCH --gres=gpu:1\n#SBATCH --account=dldevel\n#SBATCH --mem=1G\n#SBATCH --time=00:01:00\n\nmodule load cuda/11.8.0-gcc-11.3.1-szmxwyv\nnvcc --version\necho \"Finished execution\"\n</code></pre> <p>Run both scripts via <code>sbatch nvcc_11.sh</code> / <code>sbatch_nvcc_12.sh</code></p> <p>The scripts should produce the following output:</p> <p><code>nvcc_12.sh</code>:</p> <pre><code>nvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2023 NVIDIA Corporation\nBuilt on Fri_Sep__8_19:17:24_PDT_2023\nCuda compilation tools, release 12.3, V12.3.52\nBuild cuda_12.3.r12.3/compiler.33281558_0\nFinished execution\n</code></pre> <p><code>nvcc_11.sh</code></p> <pre><code>Loading cuda/11.8.0-gcc-11.3.1-szmxwyv\n  Loading requirement: gcc-runtime/11.3.1-gcc-11.3.1-5wigvvv\n    libiconv/1.17-gcc-11.3.1-szpwjfa xz/5.4.6-gcc-11.3.1-atuphiv\n    zlib-ng/2.1.5-gcc-11.3.1-onwqkc5 libxml2/2.10.3-gcc-11.3.1-umgxq4p\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2022 NVIDIA Corporation\nBuilt on Wed_Sep_21_10:33:58_PDT_2022\nCuda compilation tools, release 11.8, V11.8.89\nBuild cuda_11.8.r11.8/compiler.31833905_0\nFinished execution\n</code></pre> <p>As you can see both scripts have loaded diffferent CUDA versions.</p>"},{"location":"Developers/WSL/","title":"Windows Subsystem for Linux","text":"<p>If you want to train deep learning models under Windows in a Linux environment you can use the Windows Subsystem for Linux (WSL2). The advantage of WSL2 about a virtual machine is that it can use the installed graphics drivers for your NVIDIA GPU, so you don't have to install new drivers in the Linux environment. </p> <p>Microsoft offers a nice tutorial for seetting up WSL2 on your device here: https://learn.microsoft.com/en-us/windows/wsl/install</p> <p>You should then go to the Microsoft Store app on your PC and download the Ubuntu WSL2 package. After the installation the Ubuntu terminal is available in Windows' terminal app.</p>"},{"location":"Developers/WSL/#check-if-nvidia-gpu-is-available","title":"Check if NVIDIA GPU is available","text":"<p>You can check if the Ubuntu environment can see the GPU using the following command: </p> <pre><code>nvidia-smi\n</code></pre> <p>You don't need to install any drivers becausue the WSL environments share the drivers with the Windows operating system. What needs to be installed is CUDA. We show you how you can do this and how you can use different CUDA versions per project using Conda. </p> <p>Here is a nice article how GPU virtualization with WSL works:</p> <p>https://developer.nvidia.com/cuda/wsl</p> <p></p>"},{"location":"Developers/WSL/#install-cuda-using-conda","title":"Install CUDA using Conda","text":"<p>You can install CUDA versions using the Conda package manager. Nvidia has an article about how you can do this here:</p> <p>https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#installing-cuda-using-conda</p> <p>The available CUDA packages can be found here:</p> <p>Available CUDA packages: https://anaconda.org/nvidia/cuda/labels</p> <p>For installing a specific CUDA version in your Conda environment just run on of these commands:</p> <pre><code># CUDA 11.8\n$ conda install cuda -c nvidia/label/cuda-11.8.0\n# CUDA 12.4\n$ conda install cuda -c nvidia/label/cuda-12.4.0\n</code></pre> <p>Sometimes executing Python code doesn't work after installing CUDA. You will get the following error message:</p> <pre><code>ModuleNotFoundError: No module named '_sysconfigdata_x86_64_conda_cos7_linux_gnu'\n</code></pre> <p>The solution is to close the terminal and reopen it again.</p>"},{"location":"Model%20Training/1-Detection%20Models/","title":"Detection Models","text":"<p>In this article we go through the steps for installing the neccessary libraries for training a model.</p>"},{"location":"Model%20Training/1-Detection%20Models/#setup","title":"Setup","text":"<p>Before you can train the models you need to install the necessary libraries. We assume you have already created a Conda environment. If not here is how you can create a new one:</p> <pre><code>$ pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n$ pip install -U openmim\n$ mim install mmengine mmcv==2.1\n$ pip install -v -e .\n$ pip install -v -e . -r requirements/tracking.txt\n$ pip install globox lap git+https://github.com/JonathonLuiten/TrackEval.git\n</code></pre>"},{"location":"Model%20Training/1-Detection%20Models/#model-training","title":"Model Training","text":"<p>In this article we will show you how you can use this repository for training models that can be used for SpineUI.</p>"},{"location":"Model%20Training/1-Detection%20Models/#training","title":"Training","text":"<p>For training there is a CLI written with the Python Click framework which you can use. For simplicity you can create a Bash file that calls this API so that you do not have to type the whole command each time you want to train a model.</p> <p>Create a file named <code>train.sh</code> with the following content in the directory where the CLI is located.</p> <pre><code>export CONFIG_DIR=configs\nexport DATASET_DIR=data/spine\nexport ANNOTATIONS_TRAIN=$DATASET_DIR/annotations/instances_train2017.json\nexport ANNOTATIONS_VAL=$DATASET_DIR/annotations/instances_val2017.json\nexport ANNOTATIONS_TEST=$DATASET_DIR/annotations/instances_test2017.json\nexport IMAGES_TRAIN=$DATASET_DIR/train2017\nexport IMAGES_VAL=$DATASET_DIR/val2017\nexport IMAGES_TEST=$DATASET_DIR/test2017\nexport CLASSES=\"classes.txt\"\nexport WORKERS=4\nexport BATCH_SIZE=16\nexport MODEL_TYPE=faster_rcnn\nexport MODEL_NAME=faster_rcnn_r50_fpn_1x_coco\nexport EPOCHS=25\nexport WORK_DIR=runs/$MODEL_TYPE/$MODEL_NAME\n\npython cli.py train \\\n    --config_dir $CONFIG_DIR \\\n    --train_annotations $ANNOTATIONS_TRAIN \\\n    --train_images $IMAGES_TRAIN \\\n    --val_annotations $ANNOTATIONS_VAL \\\n    --val_images $IMAGES_VAL \\\n    --test_annotations $ANNOTATIONS_TEST \\\n    --test_images $IMAGES_TEST \\\n    --model_type $MODEL_TYPE \\\n    --model_name $MODEL_NAME \\\n    --epochs $EPOCHS \\\n    --classes classes.txt \\\n    --batch_size $BATCH_SIZE \\\n    --work_dir $WORK_DIR\n</code></pre>"},{"location":"Model%20Training/1-Detection%20Models/#train-faster-rcnn","title":"Train Faster-RCNN","text":"ResNet50 BackboneResNet101 Backbone <pre><code>python tools/train.py \\\n    configs/faster_rcnn/faster-rcnn_r50_fpn_1x_coco.py \\\n    --resume \\\n    --cfg-options \\\n    train_cfg.max_epochs=12 \\\n    optim_wrapper.optimizer.lr=0.01 \\\n    default_hooks.logger.interval=10 \\\n    model.roi_head.bbox_head.num_classes=1 \\\n    train_dataloader.batch_size=16 \\\n    val_dataloader.batch_size=16 \\\n    test_dataloader.batch_size=16 \\\n</code></pre> <pre><code>python tools/train.py \\\n    configs/faster_rcnn/faster-rcnn_x101-64x4d_fpn_1x_coco.py \\\n    --resume \\\n    --cfg-options \\\n    train_cfg.max_epochs=25 \\\n    optim_wrapper.optimizer.lr=0.05 \\\n    default_hooks.logger.interval=10 \\\n    model.roi_head.bbox_head.num_classes=1 \\\n    train_dataloader.batch_size=8 \\\n    val_dataloader.batch_size=8 \\\n    test_dataloader.batch_size=8 \\\n</code></pre>"},{"location":"Model%20Training/1-Detection%20Models/#train-cascade-rcnn","title":"Train Cascade-RCNN","text":""},{"location":"Model%20Training/1-Detection%20Models/#train-deformable-detr","title":"Train Deformable DETR","text":""},{"location":"Model%20Training/1-Detection%20Models/#train-co-detr","title":"Train Co-DETR","text":""},{"location":"Model%20Training/1-Detection%20Models/#train-yolox","title":"Train YoloX","text":"<p>After training the results will be saved in <code>./runs/&lt;model_name&gt;/&lt;submodel_name&gt;</code>. You can find the model weights as <code>.pth</code> files for each epochs and the training log files there. Also there is a file named <code>&lt;model_name&gt;.py</code> which contains all the parameters used for training the model.</p>"},{"location":"Model%20Training/1-Detection%20Models/#model-evaluation","title":"Model Evaluation","text":"<p>In this artice we will show you how you can evaluate the models you have trained. Evaluation means that you get accuracy, precision, recall and F1-scores for each epoch of your training process.</p>"},{"location":"Model%20Training/1-Detection%20Models/#evaluation","title":"Evaluation","text":"<p>You can use the CLI again for evaluating your trained model. If you want to evaluate the model for each of the datasets (train / val / test) you can do this with the following command:</p> <pre><code>export CONFIG_DIR=configs\n\nexport DATASET_DIR=data/spine\nexport CLASSES=\"classes.txt\"\nexport ANNOTATIONS_TRAIN=$DATASET_DIR/annotations/instances_train2017.json\nexport ANNOTATIONS_VAL=$DATASET_DIR/annotations/instances_val2017.json\nexport ANNOTATIONS_TEST=$DATASET_DIR/annotations/instances_test2017.json\nexport IMAGES_TRAIN=$DATASET_DIR/train2017\nexport IMAGES_VAL=$DATASET_DIR/val2017\nexport IMAGES_TEST=$DATASET_DIR/test2017\n\nexport MODEL_TYPE=faster_rcnn\nexport MODEL_NAME=faster_rcnn_x101_64x4d_fpn_1x_coco\nexport BATCH_SIZE=8\nexport EPOCHS=25\nexport WORK_DIR=runs/$MODEL_TYPE/$MODEL_NAME\n\n# Evaluate detected bounding boxes for all datasets\npython cli.py eval \\\n    --model_type $MODEL_TYPE \\\n    --model_name $MODEL_NAME \\\n    --annotations $ANNOTATIONS_TRAIN \\\n    --epochs $EPOCHS \\\n    --csv_file_pattern detections_train_epoch_\\$i.csv \\\n    --results_file eval_${MODEL_NAME}_train.csv\n\n# Evaluate detected bounding boxes for validation dataset\npython cli.py eval \\\n    --model_type $MODEL_TYPE \\\n    --model_name $MODEL_NAME \\\n    --annotations $ANNOTATIONS_VAL \\\n    --epochs $EPOCHS \\\n    --csv_file_pattern detections_val_epoch_\\$i.csv \\\n    --results_file eval_${MODEL_NAME}_val.csv\n\n# Evaluate detected bounding boxes for test dataset\npython cli.py eval \\\n    --model_type $MODEL_TYPE \\\n    --model_name $MODEL_NAME \\\n    --annotations $ANNOTATIONS_TEST \\\n    --epochs $EPOCHS \\\n    --csv_file_pattern detections_test_epoch_\\$i.csv \\\n    --results_file eval_${MODEL_NAME}_test.csv\n</code></pre> <p>This will create CSV files with names <code>eval_&lt;model_name&gt;_&lt;dataset_name&gt;.csv</code> in the directory <code>./runs/&lt;model_name&gt;/&lt;submodel_name&gt;</code>. The contents of this CSV will look like this:</p> <pre><code>ap,ar,f1,ap_50,ap_75,ap_small,ap_medium,ap_large,ar_1,ar_10,ar_100,ar_small,ar_medium,ar_large\n0.018238665971860343,0.018142235123367198,0.018190322748297905,0.018238665971860343,0.018238665971860343,0.0177267288132322,0.0,,0.011611030478955007,0.0171988388969521,0.0171988388969521,0.017362637362637365,0.0,\n0.5607005189383254,0.6262699564586357,0.5916741770083266,0.5607005189383254,0.5214491508295125,0.45215635099911644,0.6160537482319663,,0.11589259796806968,0.4957910014513788,0.5521770682148041,0.5517948717948717,0.6461538461538461,\n0.6702040721260409,0.762699564586357,0.7134664758997078,0.6702040721260409,0.6223362383788008,0.5603092855567239,0.5349535368283374,,0.12017416545718436,0.5731494920174166,0.6701015965166909,0.6706227106227107,0.6538461538461539,\n</code></pre> <p>It contains the following metrics for each epoch:</p> <ul> <li>Precision</li> <li>Recall</li> <li>F1-Score</li> <li>AP50 score</li> <li>AP75 score</li> <li>AP small score</li> <li>AP medium score</li> <li>AP large score</li> <li>AR1 score</li> <li>AR10 score</li> <li>AR100 score</li> <li>AR small score</li> <li>AR medium score</li> <li>AR large score</li> </ul>"},{"location":"Model%20Training/1-Detection%20Models/#inference","title":"Inference","text":"<p>In this article we will show you how you can evaluate your trained model.</p>"},{"location":"Model%20Training/1-Detection%20Models/#predict-bounding-boxes","title":"Predict Bounding Boxes","text":"<p>Like for training you can also use the CLI for evaluating the model. We assume that you trained the model and that there are <code>.pth</code> files for each epoch in the directory <code>./runs/&lt;model_name&gt;/&lt;submodel_name&gt;</code>.</p> <p>The first step is to use the model for predicting bounding boxes for all images in the dataset. If you want to predict the bounding boxes for a given epoch and one of the three datasets (train / val / test) you can do this with the following command:</p> <pre><code># Detect bounding boxes for the first epoch of the training dataset\nexport DATASET_DIR=data/spine\nexport DATASET=train\nexport MODEL_TYPE=faster_rcnn\nexport MODEL_NAME=faster_rcnn_x101_64x4d_fpn_1x_coco\nexport BATCH_SIZE=8\nexport EPOCH=1\n\n\n$ python cli.py detect \\\n    --model_type $MODEL_TYPE \\\n    --model_name $MODEL_NAME \\\n    --weight_file epoch_$EPOCH.pth \\\n    --image_files $DATASET_DIR/$DATASET\"'2017/*.png'\" \\\n    --results_file detections_${DATASET}_epoch_${EPOCH}.csv \\\n    --batch_size $BATCH_SIZE \\\n    --device cuda:0\n</code></pre> <p>If you want to get predictions for all epochs and all datasets you can modify the bash script in the following way:</p> <pre><code># Detect bounding boxes for all datasets and epochs\nexport DATASET_DIR=data/spine\nexport MODEL_TYPE=faster_rcnn\nexport MODEL_NAME=faster_rcnn_x101_64x4d_fpn_1x_coco\nexport BATCH_SIZE=8\n\nfor DATASET in \"train\" \"val\" \"test\"\ndo\n    for EPOCH in $(seq 1 $EPOCHS)\n    do\n        python cli.py detect \\\n            --model_type $MODEL_TYPE \\\n            --model_name $MODEL_NAME \\\n            --weight_file epoch_$EPOCH.pth \\\n            --image_files $DATASET_DIR/$DATASET\"'2017/*.png'\" \\\n            --results_file detections_${DATASET}_epoch_${EPOCH}.csv \\\n            --batch_size $BATCH_SIZE \\\n            --device cuda:0\n    done\ndone\n</code></pre> <p>When running the <code>detect</code> command a CSV file containing the bounding boxes will be created for each dataset and epoch. You can find these CSV files in the directory <code>./runs/&lt;model_name&gt;/&lt;submodel_name&gt;</code>.</p> <p>The content of the CSV file will look like this:</p> <pre><code>filename,class_index,class_name,xmin,ymin,xmax,ymax,score\naidv853_date220321_tp1_stack1_sub22_layer081.png,0,spine,161,400,180,420,0.16251738369464874\naidv853_date220321_tp1_stack1_sub22_layer081.png,0,spine,499,87,512,107,0.14402839541435242\naidv853_date220321_tp2_stack0_sub22_layer025.png,0,spine,38,82,59,97,0.2981097996234894\n</code></pre> <p>It contains the following information:</p> <ul> <li>Filename</li> <li>Numeric class index and class name of the bounding box</li> <li>Coordinates in XYXY format</li> <li>Confidence Score</li> </ul> <p>The detection CSV files can now be used in the SpineUI app for visually showing the detected bounding boxes of the model.</p>"},{"location":"Model%20Training/2-Tracking%20Models/","title":"Tracking Models","text":"<p>conda create -n mmtracking python==3.7.16 pip conda activate mmtracking</p> <pre><code>pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117\n</code></pre>"},{"location":"Model%20Training/2-Tracking%20Models/#install-mmdet","title":"Install mmdet","text":"<pre><code>pip install -U openmim\nmim install mmengine==0.10.4 mmcv-full==1.6.2 mmdet==2.28.2 mmtrack==0.14.0\npip install yapf==0.40.1\npip install git+https://github.com/JonathonLuiten/TrackEval.git\npip install -v -e .\n</code></pre>"},{"location":"Model%20Training/2-Tracking%20Models/#clone-repository","title":"Clone Repository","text":"<pre><code>git clone https://github.com/open-mmlab/mmtracking.git\n</code></pre>"},{"location":"Model%20Training/2-Tracking%20Models/#verify-installation","title":"Verify installation","text":""},{"location":"Model%20Training/2-Tracking%20Models/#with-cuda","title":"with CUDA","text":"<pre><code>python demo/demo_mot_vis.py configs/mot/deepsort/sort_faster-rcnn_fpn_4e_mot17-private.py --input demo/demo.mp4 --output mot.mp4\n</code></pre>"},{"location":"Model%20Training/2-Tracking%20Models/#without-cuda","title":"without CUDA","text":"<pre><code>python demo/demo_mot_vis.py configs/mot/deepsort/sort_faster-rcnn_fpn_4e_mot17-private.py --input demo/demo.mp4 --output mot.mp4 --device cpu\n</code></pre>"},{"location":"Model%20Training/2-Tracking%20Models/#train-mot-model","title":"Train MOT model","text":""},{"location":"Model%20Training/2-Tracking%20Models/#convert-dataset-from-mot-to-coco","title":"Convert dataset from MOT to COCO","text":"<pre><code>python ./tools/dataset_converters/mot2reid.py -i ./data/MOT17/ -o ./data/MOT17/reid --val-split 0.2 --vis-threshold 0.3\n</code></pre>"},{"location":"Model%20Training/2-Tracking%20Models/#training","title":"Training","text":"<pre><code>python tools/train.py ./configs/det/faster-rcnn_r50_fpn_4e_mot17.py\n</code></pre>"},{"location":"Model%20Training/2-Tracking%20Models/#train-reid-model","title":"Train REID model","text":""},{"location":"Model%20Training/2-Tracking%20Models/#convert-dataset-from-mot-to-reid","title":"Convert dataset from MOT to REID","text":"<pre><code>python ./tools/convert_datasets/mot/mot2reid.py -i ./data/MOT17/ -o ./data/MOT17/reid --val-split 0.2 --vis-threshold 0.3\n</code></pre>"},{"location":"Model%20Training/2-Tracking%20Models/#training_1","title":"Training","text":"<pre><code>python ./tools/train.py ./configs/reid/resnet50_b32x8_MOT17.py\n</code></pre>"},{"location":"Model%20Training/2-Tracking%20Models/#sort-deepsort-tracktor-inference","title":"SORT, DeepSort, Tracktor Inference","text":"<p>Make a copy of one of the configuration file for the algorithm. You can find these configuration files in these directories:</p> <p>First change the line where the checkpoint for the detector model is defined:</p> <pre><code>model = dict(\n    [...]\n    detector=dict(\n        [...]\n        init_cfg=dict(\n            [...]\n            checkpoint='./work_dirs/faster-rcnn_r50_fpn_4e_mot17/epoch_4.pth'\n</code></pre> <p>Next change the line where the checkpoint for the REID model is defined:</p> <pre><code>model = dict(\n    [...]\n    reid=dict(\n        [...]\n        init_cfg=dict(\n            [...]\n            checkpoint='./work_dirs/resnet50_b32x8_MOT17/epoch_6.pth'\n</code></pre> <p>This will make the tool use your own detector and REID model instead of downloading a pretrained model for inference.</p> <p>Now use this config file for inference with the following command:</p> <pre><code>python ./demo/demo_mot_vis.py ./configs/mot/&lt;algorithm_name&gt;/&lt;your_config_file&gt;.py --input ./data/MOT17/train/stack1/img --output mydemo.mp4 --fps 1 --device cpu\n</code></pre>"},{"location":"Model%20Training/2-Tracking%20Models/#trackeval-usage","title":"TrackEval Usage","text":""},{"location":"Model%20Training/2-Tracking%20Models/#ground-truths","title":"Ground Truths","text":"<p>Within your project there should be a directories named <code>data/MOT17/&lt;train|test&gt;</code> with the following structure:</p> <pre><code>data/MOT17\n|- &lt;train|test&gt;\n|-|- &lt;stack_name_1&gt;\n|-|-|- gt\n|-|-|-|- gt.txt\n|-|-|- img\n|-|-|-|- 000001.png\n|-|-|-|- 000002.png\n|-|-|-|- [...]\n|-|-|- seqinfo.ini\n|-|- &lt;stack_name_2&gt;\n|-|-|- [...]\n|-|- &lt;stack_name_3&gt;\n|-|-|- [...]\n|-|- seqmaps.txt\n</code></pre> <p>The file <code>seqmaps.txt</code> contains the names of all stacks in the following CSV format:</p> <pre><code>name\n&lt;stack_name_1&gt;\n&lt;stack_name_2&gt;\n[...]\n</code></pre> <p>The first line is the header line and then for each stack a new line containing only the name of the stack is added. The stack names are the same as the directory names within <code>&lt;train|test&gt;</code>.</p> <p>The file <code>gt.txt</code> contains the ground truth in the following format:</p> <p><code>&lt;frame&gt;,&lt;object_id&gt;,&lt;x0&gt;,&lt;y0&gt;,&lt;width&gt;,&lt;height&gt;,&lt;confidence&gt;,&lt;x&gt;,&lt;y&gt;,&lt;z&gt;</code></p> <p>Here is an example of how this could look like:</p> <pre><code>1,3,230,479,13,11,1.0,-1,-1,-1\n2,3,230,478,15,17,1.0,-1,-1,-1\n2,2,225,435,14,16,1.0,-1,-1,-1\n</code></pre> <p>You can find more about the MOT17 format here: https://motchallenge.net/instructions/</p> <p>The file <code>seqinfo.ini</code> contains the info about the stack in the following format:</p> <pre><code>[Sequence]\nname=&lt;stack_name&gt;\nimDir=img\nframeRate=1\nseqLen=20\nimWidth=512\nimHeight=512\nimExt=.png\nseqLength=20\n</code></pre>"},{"location":"Model%20Training/2-Tracking%20Models/#detections","title":"Detections","text":"<p>When you want to evaluate a tracking model using metrics like MOTA or HOTA create a directory named <code>results/&lt;model_name&gt;</code> with the following structure:</p> <pre><code>results/\n|- &lt;model_name&gt;\n|-|- &lt;seqmaps&gt;\n|-|-|- &lt;stack_name_1&gt;.txt\n|-|-|- &lt;stack_name_2&gt;.txt\n|-|-|- [...]\n</code></pre> <p>For each stack a text file with name <code>&lt;stack_name&gt;.txt</code> is created in the directory <code>results/&lt;model_name&gt;/seqmaps</code> is created with the same format as the ground truth data. Here is an example how the content of these files could look like:</p> <pre><code>1,3,232,481,14,9,0.65,-1,-1,-1\n2,3,233,475,15,18,0.89,-1,-1,-1\n2,2,223,431,16,16,0.71,-1,-1,-1\n</code></pre> <p>The values of <code>x0</code>, <code>y0</code>. <code>height</code>, <code>width</code> and <code>confidence</code> are those that were predicted by the model. If you have multiple models you can create a directory in <code>results</code> for each model.</p> <p>Tracking metrics can now be computed via the CLI in the following way:</p> <pre><code>python cli.py eval-tracking \\\n    --gt-folder data/MOT17/train \\\n    --detections results/&lt;model_name&gt;/ \\\n    --output-dir tracking-results\n</code></pre> <p>After running this command you will find plots and CSV files containing the results in the directory <code>training-results</code>. Also detailed information about tracking metrics is printed to the console.</p>"},{"location":"Model%20Training/3-Trackformer/","title":"Trackformer","text":"<p>Trackformer is an end-to-end object tracking model. You can find the source code inside the <code>trackformer</code> directory of this project.</p> <p>You cn find the source code we used for training the trackformer model in the <code>repositories/trackformer</code> directory of this Git repository.</p>"},{"location":"Model%20Training/3-Trackformer/#environment","title":"Environment","text":"<p>Trackformer was built to run in an environment where Python 3.7.x should be installed. There are several options for initializing such an environment on your system. We will show you two ways how you can setup this environment on your machine using Conda or Docker.</p>"},{"location":"Model%20Training/3-Trackformer/#setup-with-wsl","title":"Setup with WSL","text":"<p>You can setup Trackformer in a WSL environment under Windows. We have a separate article in this documentation about installing Linux CUDA environments under Windows. Follow the steps of this article and then follow the installation process described below for Conda environments.</p>"},{"location":"Model%20Training/3-Trackformer/#setup-with-docker-on-windows","title":"Setup with Docker on Windows","text":"<p>It is possible to run Trackformer inside a Docker container on Windows. We will explain the stept for creating the Docker image and how to run this Docker image in the following section.</p>"},{"location":"Model%20Training/3-Trackformer/#spine-dataset","title":"Spine dataset","text":"<p>Place the Spine MOT dataset in this directory: <code>data/spine</code>. The directory structure should now look like this:</p> <pre><code>data\n|- spine\n  |- annotations\n    |- test.json\n    |- train.json\n    |- val.json\n  |- test\n    |- &lt;stack_name&gt;\n      |- det\n      |- gt\n      |- img\n      |- seqinfo.ini\n    |- &lt;stack_name&gt;\n      |- ...\n  |- train\n    |- &lt;stack_name&gt;\n      |- det\n      |- gt\n      |- img\n      |- seqinfo.ini\n    |- &lt;stack_name&gt;\n      |- ...\n  |- val\n    |- &lt;stack_name&gt;\n      |- det\n      |- gt\n      |- img\n      |- seqinfo.ini\n    |- &lt;stack_name&gt;\n      |- ...\n</code></pre>"},{"location":"Model%20Training/3-Trackformer/#build-the-docker-image","title":"Build the Docker image","text":"<p>First we need to build the docker image. We have included a Dockerfile inside this repository under <code>trackformer/Dockerfile</code>. For building the image run the following command:</p> <pre><code>$ docker build -t tnodecode/trackformer-base\n</code></pre> <p>This process can take a while because the base image used is very large. After the command has finished you can now create an instance of this docker image and open its shell with the following command:</p> <pre><code>docker run \\\n    -it \\\n    --gpus all \\\n    --shm-size=16gb \\\n    --rm \\\n    tnodecode/trackformer-base \\\n    bash\n</code></pre> <p>Your terminal should now look like this:</p> <pre><code>root@3139f5fc0c8d:/app#\n</code></pre> <p>This means you are using the shell of your docker container. It is similar to connecting to a remote machine via SSH.</p> <p>Now we have created a Docker image that contains a PyTorch runtime together with the CUDA development tools which are needed for compiling the CUDA operators provided in <code>src/trackformer/models/ops</code>. Unfortunately Docker cannot access the GPU during image built time, so we have install the CUDA runtime and compie the operators via the Docker container shell. </p> <p>In the docker container shell run the following two commands and DO NOT CLOSE the shell afterwards.</p> <pre><code># First switch to the conda trackformer environment inside the container\n$ conda init\n$ source ~/.bashrc\n$ conda activate trackformer\n# Install NVIDIA CUDA toolkit\n$ apt-get install -y nvidia-cuda-dev nvidia-cuda-toolkit\n# Compile the operators\n$ python src/trackformer/models/ops/setup.py build --build-base=src/trackformer/models/ops/ install\n# Update numpy, scipy and pandas\n$ pip install -U numpy scipy pandas\n</code></pre> <p>When both commands are executed open a second terminal while the docker shell is still opened. In the second terminal type <code>docker ps</code> to get a list of all running docker containers. Find the container that you started and its name (<code>clever_bhabha</code> in this case).</p> <pre><code>$ docker ps\nCONTAINER ID   IMAGE                   COMMAND   CREATED         STATUS         PORTS     NAMES\n3139f5fc0c8d   tnodecode/trackformer-base   \"bash\"    2 minutes ago   Up 2 minutes             clever_bhabha\n</code></pre> <p>Now we want to create a new docker image based on our currently running container. We can do this with the following command:</p> <pre><code>$ docker commit clever_bhabha tnodecode/trackformer\n</code></pre> <p>This will create a new docker image with tag <code>tnodecode/trackformer</code> based on the container with the name <code>clever_bhabha</code>. After this command you can close the docker container shell in the first terminal by simply typing \"exit\" and pressing enter.</p> <p>Now you can create a container that is based on our new image by executing the following command:</p> <pre><code>$ docker run \\\n    -it \\\n    --gpus all \\\n    --shm-size=16gb \\\n    --rm \\\n    tnodecode/trackformer \\\n    bash\n</code></pre>"},{"location":"Model%20Training/3-Trackformer/#train-model-on-windows-with-docker","title":"Train model on Windows with Docker","text":"<p>Now that we have built the image we can start training a Trackformer model. First download the pretrained models and place them inside the <code>models</code> directory. Then place the dataset inside the <code>data/spine</code> directory. Also create a directory named <code>checkpoints</code> where trackformer will store the ResNet50 or ResNet101 pretrained models.</p> <p>Now you can run the following command:</p> <pre><code>docker run \\\n    --gpus all \\\n    --shm-size=16gb \\\n    --rm \\\n    -v $PWD/cfgs:/app/cfgs \\\n    -v $PWD/data:/app/data \\\n    -v $PWD/models:/app/models \\\n    -v $PWD/src:/app/src \\\n    -v $PWD/checkpoints:/root/.cache/torch/hub/checkpoints \\\n    tnodecode/trackformer \\\n    conda run --no-capture-output -n trackformer \\\n    python src/train.py with \\\n    mot17 \\\n    deformable \\\n    multi_frame \\\n    tracking \\\n    device=cuda:0 \\\n    output_dir=checkpoints/custom_dataset_deformable \\\n    mot_path_train=data/spine \\\n    mot_path_val=data/spine \\\n    train_split=train \\\n    val_split=val \\\n    epochs=20 \\\n</code></pre> <p>The option <code>-v $PWD/cfgs:/app/cfgs</code> maps a directory from Windows into the container. PWD is the absolute path of your project (C:/Users//). We do this so that we can make changes to the code or the dataset without rebuilding the docker image. Also the created model weights are stored on our Windows filesystem instead on the container filesystem that will be destroyed when the container is shut down. The directory <code>/root/.cache/torch/hub/checkpoints</code> inside the container stores the downloaded models from Torch Hub. If we wouldn't map this directory to the <code>checkpoints</code> directory of our project the conainer woud download the model each time we restart the container because its filesystem is not persisted on the disk."},{"location":"Model%20Training/3-Trackformer/#train-a-model-on-a-linux-machine","title":"Train a model on a Linux machine","text":"<p>Training the model on a linux machine can be done by the following command:</p> <pre><code>python src/train.py with \\\n    mot17 \\\n    deformable \\\n    multi_frame \\\n    tracking \\\n    device=cuda:0 \\\n    output_dir=checkpoints/custom_dataset_deformable \\\n    mot_path_train=data/spine \\\n    mot_path_val=data/spine \\\n    train_split=train \\\n    val_split=val \\\n    epochs=20 \\\n</code></pre> <p>We also created a script named <code>train_custom.sh</code> in the root directory of the trackformer repository which contains this command.</p>"},{"location":"Model%20Training/3-Trackformer/#inference","title":"Inference","text":"<p>Inference for all stacks of a dataset for all splits can be done with the following bash script:</p> <pre><code>export DATASET=spine        # dataset name\nexport SUBDIR=run_1         # subdir in checkpoints dir\n\nfor MODEL_NAME in MOTA IDF1 BBOX_AP_IoU_0_50\ndo\n    for SPLIT in train val test\n    do\n        for file in ./data/$DATASET/$SPLIT/*\n        do\n        if [ -d \"$file\" ]; then\n            STACK_NAME=\"$(basename -- $file)\"\n            echo \"Processing\" $SPLIT $STACK_NAME \"...\"\n            python src/track.py with \\\n                dataset_name=$STACK_NAME \\\n                obj_detect_checkpoint_file=checkpoints/$SUBDIR/checkpoint_best_$MODEL_NAME.pth \\\n                write_images=True \\\n                generate_attention_maps=False \\\n                output_dir=detections/$SUBDIR/$MODEL_NAME/$SPLIT\n        fi\n        done\n    done\ndone\n</code></pre> <p>This will run inference on all images found in <code>data/spine/$SPLIT</code> and save renderen images with bounding boxes and trackformer detection files in <code>detections/$SUBDIR/$MODEL_NAME/$SPLIT</code>.</p> <p>The repository also contains this bash code in the file <code>track_custom.sh</code> in the root directory.</p>"},{"location":"Setup/Installation/","title":"Setup SpineUI","text":"<p>There are two possible ways to setup SpineUI on your machine. The first way is to use the prebuilt Docker images, which is the way you should prefer if you are not a developer. The second way is to set it up on your machine without using Docker which will require more technical skills.</p>"},{"location":"Setup/Installation/#using-docker","title":"Using Docker","text":""},{"location":"Setup/Installation/#software-requirements","title":"Software Requirements","text":"<p>For using SpineUI you need to have installed Docker on your machine.</p>"},{"location":"Setup/Installation/#how-to-start-the-application","title":"How to start the application","text":"<p>You can start the appliction by running the following command: <code>docker compose up -d</code>. When you run the appliaction Docker will download all the required images needed for running this application. It will download one image for the UI and one image per model that is being used.</p>"},{"location":"Setup/Installation/#shut-down-the-application","title":"Shut down the application","text":"<p>The application can be shut down by executing the command <code>docker compose down</code>.</p>"},{"location":"Setup/Installation/#without-docker","title":"Without Docker","text":""},{"location":"Setup/Installation/#prerequisites","title":"Prerequisites","text":"<p>You need to have installed a C++ compier on your machine. This is necessary because otherwise the the MMCV and MMDet libraries cannot be installed on your machine. You should also have installed Python &gt;= 3.10 on your machine as well as the PIP package manager. It is recommended to install Miniconda on your system.</p>"},{"location":"Setup/Installation/#create-a-new-conda-environment","title":"Create a new Conda environment","text":"<p>First we need to create a new Conda environment.</p> <pre><code>$ conda create -n spine python==3.11 pip\n$ conda activate spine\n</code></pre>"},{"location":"Setup/Installation/#clone-the-github-repository","title":"Clone the GitHub Repository","text":"<p>Clone the GitHub Repository by running the following commands:</p> <pre><code>$ git clone https://github.com/TNodeCode/SpineUI\n$ cd SpineUI\n</code></pre>"},{"location":"Setup/Installation/#install-necessary-libraries","title":"Install necessary libraries","text":"<p>First we need to install the PyTorch libraries. There is a good interactive tool that can hep you to build the needed installation command here for the specific environment of your machine (Windows / Mac, CUDA / CPU, etc.). But this command shoud always work if you want to use only the CPU:</p> <pre><code>$ pip install torch torchvision torchaudio\n</code></pre>"},{"location":"Setup/Installation/#install-the-mmcv-and-mmdet-libraries","title":"Install the MMCV and MMDet libraries","text":"<p>SpineUI is built on top of the OpenMMLAB platform and specifically on top of the MMCV and MMDet libraries which are part of this platform. As these libraries are constantly being further developed we included a copy of the source code in this repository that should work for SpineUI. You can find it in the subdirectory <code>mmcv</code> of this repository. You can install the MMCV and MMDet libraries with the following commands:</p> <pre><code>$ cd mmcv\n$ FORCE_CUDA=1 MMCV_WITH_OPS=1 pip install -e . -v\n</code></pre> <p>Because both libraries contain some C++ code and are compiled ahead of time we need to specify two parameters during compile time. - <code>FORCE_CUDA</code> defines if the libraries shoud be compiled with support for CUDA devices. If you want to support CUDA devices you need to set this parameter to <code>1</code>, otherwise set it to <code>0</code>. Also you need to have installed the Nvidia C-Compiler <code>nvcc</code> on your system as well as CUDA&gt;12.0. When compiling the code a CUDA device must be connected to your machine, otherwise compilation will not work if the parameter is set to 1. - <code>MMCV_WITH_OPS</code> defines whether operators should be compiled. This parameter should be always set to <code>1</code>. Otherwise operators won't be compiled and your models are not optimized for the plattform you are running them on. You can do inference without compiling the operators but not training.</p>"},{"location":"Setup/Installation/#install-other-python-libraries","title":"Install other Python libraries","text":"<p>Now we need to install some more Python libraries. The needed Python libraries are defined in the file <code>requirements.txt</code>. You can install them by running the following command:</p> <pre><code>$ pip install -r requirements.txt\n</code></pre>"},{"location":"Setup/Installation/#common-errors-when-installing-mmcv","title":"Common errors when installing MMCV","text":"<pre><code>ModuleNotFoundError: No module named 'mmcv._ext'\n</code></pre> <p>This means that the MMCV operators were not compiled, which is ok for inference but not for training. The mmcv operators are actually written in C++ for performance reasons. Python can call compiled C++ code via the <code>_ext</code> interface. The solution is to first uninstall the existing mmcv version with the command pip uninstall mmcv and then reinstall the library using this command: <code>FORCE_CUDA=0 MMCV_WITH_OPS=1 pip install -e . -v</code>. If CUDA is available on your system then you should set <code>FORCE_CUDA</code> to <code>1</code>. Then the operators are also compiled against the nvcc CUDA compiler and can be executed on any CUDA hardware.</p>"},{"location":"Setup/Installation/#installing-a-c-compiler","title":"Installing a C++ compiler","text":""},{"location":"Setup/Installation/#on-linux","title":"On Linux","text":"<p>On Linux it is pretty easy to install a C++ compiler. Run the following command:</p> <pre><code>$ sudo apt-get update &amp;&amp; sudo apt-get install -y g++\n</code></pre>"},{"location":"Setup/Installation/#on-windows","title":"On Windows","text":"<p>On Windows machines you need to have instaled the Visual Studio Community edition. You can install a compiler by installing the Microsoft Build Tools on your PC (Microsoft Build Tools f\u00fcr C++ - Visual Studio). Switch to the \u201csingle compoents\u201d tab in the installer and check the option \"Windows 11 SDK\" (or \"Windows 10 SDK\" if you are working with Windows 10).</p> <p></p>"}]}